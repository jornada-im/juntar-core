{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIS lateral photo and cover data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lat_path = \"/media/greg/jrn-DataArchive/Data_ENT/LTER/_Entry/CSIS/Laterals/\"\n",
    "dest_path1 = \"/media/greg/jrn-DataProducts/JORNADA_IM/WIP_packages/210413006_CSIS_lateral_photos/\"\n",
    "dest_path2 = \"/media/greg/jrn-DataProducts/JORNADA_IM/WIP_packages/210413007_CSIS_lateral_area/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory the lateral photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lateral photos in folder: 6154\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the jpeg files in CSIS/Laterals\n",
    "fns = [glob.glob(os.path.join(lat_path,'**/*.{0}'.format(e)), recursive=True) for e in ['JPG', 'jpg', 'JPEG', 'jpeg']]\n",
    "# Join the nested list\n",
    "fns = sum(fns, []) # If we end up with list of lists\n",
    "print('number of lateral photos in folder: ' + str(len(fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"all lateral photos\" dataframe\n",
    "fnames = [os.path.basename(fn) for fn in fns]# Pull off just filename\n",
    "latphotos_df = pd.DataFrame({'pathname':fns, 'fname':fnames, 'ptype':'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lateral duplicates: 168\n"
     ]
    }
   ],
   "source": [
    "# find duplicate photo filenames - this could be problematic\n",
    "# for locating source files for area estimates\n",
    "ndups =  latphotos_df[latphotos_df.fname.duplicated()]\n",
    "print('number of lateral duplicates: ' + str(len(ndups)))\n",
    "# Get all duplicate rows (see https://stackoverflow.com/a/14657511)\n",
    "dups = latphotos_df[latphotos_df.duplicated('fname', keep=False) == True]\n",
    "dups.to_csv(os.path.join(dest_path1, 'lat_dups.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the litter area/accum files\n",
    "\n",
    "From SigmaScan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "5996\n"
     ]
    }
   ],
   "source": [
    "# Get the area files from SigmaScan, loop through, and load\n",
    "areafiles = glob.glob(os.path.join(lat_path, '_CSIS_lateral-photos_Area-analyses', '**/area-analysis*.xlsx'), recursive=True)\n",
    "\n",
    "frames = []\n",
    "\n",
    "for f in areafiles:\n",
    "    #print(f)\n",
    "    df = pd.read_excel(f, 0, skiprows=4, header=0, usecols='A:H',\n",
    "        dtype={'Comment':str}, na_values=['.', ' ', '', '. ', '.  ' , '<there is no #9 microplot>', '<there is no #10 microplot>'])\n",
    "    # Rename columns\n",
    "    df.columns = ['image_num_orig', 'drop1', 'image_fname', 'area_cm2', 'cal_dist_cm',\n",
    "        'drop2', 'drop3', 'comment']\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate dfs into one\n",
    "latarea_df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "# Strip some whitespace in column index and columns (not sure if necessary...)\n",
    "latarea_df.columns = latarea_df.columns.str.strip()\n",
    "latarea_df['image_fname'] = latarea_df['image_fname'].str.strip()\n",
    "latarea_df['comment'] = latarea_df['comment'].str.strip()\n",
    "# In 2017 the block 7 photos have a weird 'M: ' prepended - remove\n",
    "latarea_df['image_fname'] = latarea_df['image_fname'].str.replace('M: ', '')\n",
    "\n",
    "# Drop rows with errors (~4)\n",
    "# Basically if filename reads as NA (see read_excel params above) we can drop\n",
    "print(len(latarea_df))\n",
    "latarea_df = latarea_df[~latarea_df.image_fname.isna()]\n",
    "print(len(latarea_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the strings in \"image_fname\" by - and _\n",
    "splits1 = latarea_df['image_fname'].str.split('[-_]', expand=True)\n",
    "# Extract the block, plot, and microplots\n",
    "latarea_df['block'] = splits1[0].str.extract('(\\d+)')\n",
    "latarea_df['plot']  = splits1[0].str.extract('([a-zA-Z]+)')\n",
    "latarea_df['microplot'] = splits1[1].str.extract('(\\d+)')\n",
    "# There are two photo naming formats with dates\n",
    "# Extract (2 re exp's) dates, standardize, convert to datetimes\n",
    "latarea_df['image_date'] = latarea_df.image_fname.str.extract(r'(20\\d{2}-\\d{2}-\\d{2}|20\\d{6})')\n",
    "latarea_df['image_date'] = latarea_df.image_date.str.replace('-','')\n",
    "latarea_df['image_date'] = pd.to_datetime(latarea_df.image_date, format='%Y%m%d')\n",
    "# Type/direction\n",
    "latarea_df['photo_type'] = 'lat'\n",
    "latarea_df['dir_facing']  = splits1[1].str.extract('([a-zA-Z]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_num_orig</th>\n",
       "      <th>drop1</th>\n",
       "      <th>image_fname</th>\n",
       "      <th>area_cm2</th>\n",
       "      <th>cal_dist_cm</th>\n",
       "      <th>drop2</th>\n",
       "      <th>drop3</th>\n",
       "      <th>comment</th>\n",
       "      <th>block</th>\n",
       "      <th>plot</th>\n",
       "      <th>microplot</th>\n",
       "      <th>image_date</th>\n",
       "      <th>photo_type</th>\n",
       "      <th>dir_facing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>0.548837</td>\n",
       "      <td>15D-7N_20130301_IMG_1123764.JPG</td>\n",
       "      <td>1059.447926</td>\n",
       "      <td>50.017536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-16 08:52:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>D</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>lat</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>0.333317</td>\n",
       "      <td>15D-7S_20130301_IMG_1123766.JPG</td>\n",
       "      <td>1043.894584</td>\n",
       "      <td>50.037418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-16 08:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>D</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>lat</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>0.223188</td>\n",
       "      <td>15D-2W_20130301_IMG_1123747.JPG</td>\n",
       "      <td>1051.042591</td>\n",
       "      <td>49.960578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-16 09:06:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>lat</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0.258966</td>\n",
       "      <td>15B-3N_20130301_IMG_1123683.JPG</td>\n",
       "      <td>1167.014810</td>\n",
       "      <td>49.981310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-16 09:13:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>lat</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>0.341442</td>\n",
       "      <td>15B-8W_20130301_IMG_1123706.JPG</td>\n",
       "      <td>1173.284807</td>\n",
       "      <td>50.039988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-11-16 09:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>8</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>lat</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_num_orig     drop1                      image_fname     area_cm2  \\\n",
       "0              70  0.548837  15D-7N_20130301_IMG_1123764.JPG  1059.447926   \n",
       "1              71  0.333317  15D-7S_20130301_IMG_1123766.JPG  1043.894584   \n",
       "2              52  0.223188  15D-2W_20130301_IMG_1123747.JPG  1051.042591   \n",
       "3              14  0.258966  15B-3N_20130301_IMG_1123683.JPG  1167.014810   \n",
       "4              36  0.341442  15B-8W_20130301_IMG_1123706.JPG  1173.284807   \n",
       "\n",
       "   cal_dist_cm drop2                drop3 comment block plot microplot  \\\n",
       "0    50.017536   NaN  2018-11-16 08:52:00     NaN    15    D         7   \n",
       "1    50.037418   NaN  2018-11-16 08:59:00     NaN    15    D         7   \n",
       "2    49.960578   NaN  2018-11-16 09:06:00     NaN    15    D         2   \n",
       "3    49.981310   NaN  2018-11-16 09:13:00     NaN    15    B         3   \n",
       "4    50.039988   NaN  2018-11-16 09:20:00     NaN    15    B         8   \n",
       "\n",
       "  image_date photo_type dir_facing  \n",
       "0 2013-03-01        lat          N  \n",
       "1 2013-03-01        lat          S  \n",
       "2 2013-03-01        lat          W  \n",
       "3 2013-03-01        lat          N  \n",
       "4 2013-03-01        lat          W  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latarea_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_num_orig', 'image_fname', 'block', 'plot', 'microplot',\n",
      "       'image_date', 'photo_type', 'dir_facing', 'area_cm2', 'cal_dist_cm',\n",
      "       'comment'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5996, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder columns\n",
    "cols = list(latarea_df.columns)\n",
    "latarea_df = latarea_df[cols[0:1] + cols[2:3] + cols[-6:] + cols[3:5] + cols[7:8]]\n",
    "print(latarea_df.columns)\n",
    "# Export to csv\n",
    "latarea_df.to_csv(os.path.join(dest_path2, 'jrn413007_lat_area_data.csv'), index=False, na_rep='NA')\n",
    "latarea_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare photos analyzed with inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6154, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latphotos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(latarea_df['image_fname'].isin(latphotos_df['fname']).sum())\n",
    "# Currently there seem to be quite a few photos missing\n",
    "latarea_df.shape[0] - latarea_df['image_fname'].isin(latphotos_df['fname']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a listing of latarea Images (~6000) that are not present in the \n",
    "# overhead photo inventory (of ~6154)\n",
    "missing = latarea_df.loc[~latarea_df['image_fname'].isin(latphotos_df['fname']),\n",
    "    ['image_num_orig', 'image_fname', 'block', 'plot', 'microplot', 'image_date', 'photo_type', 'dir_facing']]\n",
    "\n",
    "missing.to_csv(os.path.join(dest_path1, 'missing_latarea_photos.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip up the photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a year column\n",
    "latarea_df['imageyear'] = latarea_df['image_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013.0\n",
      "Area data lists 1196 photos, 1280 photo paths found in archive\n",
      "2014.0\n",
      "Area data lists 1200 photos, 1196 photo paths found in archive\n",
      "2015.0\n",
      "Area data lists 1196 photos, 1196 photo paths found in archive\n",
      "2016.0\n",
      "Area data lists 1196 photos, 1196 photo paths found in archive\n",
      "2017.0\n",
      "Area data lists 1200 photos, 1284 photo paths found in archive\n",
      "                            image_fname block plot microplot image_date  \\\n",
      "5995   1C-1N_T_20170725_IMG_1106842.JPG     1    C         1 2017-07-25   \n",
      "5996  1A-8W_TH_20170725_IMG_1106797.JPG     1    A         8 2017-07-25   \n",
      "5997  1A-5N_TH_20170725_IMG_1106781.JPG     1    A         5 2017-07-25   \n",
      "5998  1A-9W_TH_20170725_IMG_1106801.JPG     1    A         9 2017-07-25   \n",
      "5999   1C-7E_T_20170725_IMG_1106867.JPG     1    C         7 2017-07-25   \n",
      "\n",
      "     photo_type dir_facing        archive_fname  \\\n",
      "5995        lat          N  lat_photos_2017.zip   \n",
      "5996        lat          W  lat_photos_2017.zip   \n",
      "5997        lat          N  lat_photos_2017.zip   \n",
      "5998        lat          W  lat_photos_2017.zip   \n",
      "5999        lat          E  lat_photos_2017.zip   \n",
      "\n",
      "                                        archive_relpath  \n",
      "5995  Block-1/versions/2018-07-23_jpa/1C-1N_T_201707...  \n",
      "5996  Block-1/versions/2018-07-23_jpa/1A-8W_TH_20170...  \n",
      "5997  Block-1/versions/2018-07-23_jpa/1A-5N_TH_20170...  \n",
      "5998  Block-1/versions/2018-07-23_jpa/1A-9W_TH_20170...  \n",
      "5999  Block-1/versions/2018-07-23_jpa/1C-7E_T_201707...  \n"
     ]
    }
   ],
   "source": [
    "# Create a directory file to populate\n",
    "photo_archive_dir = pd.DataFrame(latarea_df[['image_fname', 'block', 'plot', 'microplot','image_date', 'photo_type', 'dir_facing']])\n",
    "photo_archive_dir['archive_fname'] = 'None'\n",
    "photo_archive_dir['archive_relpath'] = 'None'\n",
    "lat_dups_used = pd.DataFrame() # Empty dataframe for duplicates\n",
    "\n",
    "# Subset by year, zip up files, fill directory, and write to JORNADA_IM directory\n",
    "import zipfile\n",
    "for y in latarea_df.imageyear.dropna().unique():\n",
    "    print(y)\n",
    "    # Subset cover dataset by year and get paths from photo inventory\n",
    "    subset = latarea_df.loc[latarea_df.imageyear==y, 'image_fname']\n",
    "    #paths = latphotos_df.loc[latphotos_df.fname.isin(subset), 'pathname']\n",
    "    # file extension case insensitive search (lots of discrepancies here)\n",
    "    lc_jpg = latphotos_df.fname.str.replace('.JPG', '.jpg').isin(subset.str.replace('.JPG', '.jpg'))\n",
    "    paths = latphotos_df.loc[lc_jpg, 'pathname']\n",
    "    print('Area data lists {0} photos, {1} photo paths found in archive'.format(\n",
    "        len(subset), len(paths)))\n",
    "    # If there are duplicate photos what are they?\n",
    "    if len(subset) < len(paths):\n",
    "        test = latphotos_df.loc[latphotos_df.fname.isin(subset), :]\n",
    "        lat_dups_used = pd.concat([lat_dups_used, test[test.duplicated('fname', keep=False) == True]])\n",
    "    # Get the parent directory of all the paths in paths (common prefix, then parent dir of that)\n",
    "    parentdir = os.path.dirname(os.path.commonprefix(paths.to_list()))\n",
    "    # Create a zipfile\n",
    "    zfile_path = os.path.join(dest_path1, 'lat_photos_{0}.zip'.format(str(int(y))))\n",
    "    #zfile = zipfile.ZipFile(zfile_path, \"w\")\n",
    "    for p in paths:\n",
    "        # Again, account for the .JPG/.jpg discrepancy\n",
    "        test = photo_archive_dir.image_fname.str.replace('.JPG', '.jpg')==os.path.basename(p.replace('.JPG', '.jpg'))\n",
    "        photo_archive_dir.loc[test, 'archive_fname'] = 'lat_photos_{0}.zip'.format(str(int(y)))\n",
    "        photo_archive_dir.loc[test, 'archive_relpath'] = os.path.relpath(p,start=parentdir)\n",
    "        # Write each file to zfile using a relative path starting at parentdir\n",
    "    #    zfile.write(p, os.path.relpath(p,start=parentdir), compress_type=zipfile.ZIP_DEFLATED)\n",
    "    #zfile.close()\n",
    "\n",
    "print(photo_archive_dir.tail())\n",
    "photo_archive_dir.to_csv(os.path.join(dest_path1, 'jrn413006_photo_archive_dir.csv'), index=False, na_rep='NA')\n",
    "lat_dups_used.to_csv(os.path.join(dest_path1, 'lat_dups_used.csv'), na_rep='NA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
