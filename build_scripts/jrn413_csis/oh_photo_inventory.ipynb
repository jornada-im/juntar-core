{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIS Overhead photo and cover data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "oh_path = \"/media/greg/jrn-DataArchive/Data_ENT/LTER/_Entry/CSIS/Overheads/\"\n",
    "dest_path1 = \"/media/greg/jrn-DataProducts/JORNADA_IM/WIP_packages/210413004_CSIS_overhead_photos/\"\n",
    "dest_path2 = \"/media/greg/jrn-DataProducts/JORNADA_IM/WIP_packages/210413005_CSIS_overhead_cover/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory the overhead photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of overhead photos in folder: 13101\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the jpeg files in CSIS/Overheads\n",
    "fns = [glob.glob(os.path.join(oh_path,'**/*.{0}'.format(e)), recursive=True) for e in ['JPG', 'jpg', 'JPEG', 'jpeg']]\n",
    "fns = sum(fns, []) # In case we end up with list of lists\n",
    "print('number of overhead photos in folder: ' + str(len(fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an \"all overhead photos\" dataframe\n",
    "fnames = [os.path.basename(fn) for fn in fns]# Pull off just filename\n",
    "ohphotos_df = pd.DataFrame({'pathname':fns, 'fname':fnames, 'ptype':'oh'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of overhead duplicates: 663\n"
     ]
    }
   ],
   "source": [
    "# find duplicate photo filenames - this could be problematic\n",
    "# for locating source files for cover estimates\n",
    "ndups =  ohphotos_df[ohphotos_df.fname.duplicated()]\n",
    "print('number of overhead duplicates: ' + str(len(ndups)))\n",
    "# Get all duplicate rows (see https://stackoverflow.com/a/14657511)\n",
    "dups = ohphotos_df[ohphotos_df.duplicated('fname', keep=False) == True]\n",
    "dups.to_csv(os.path.join(dest_path1, 'oh_dups.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the cover files\n",
    "\n",
    "From Samplepoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5982, 77)\n"
     ]
    }
   ],
   "source": [
    "# Get the cover files from SamplePoint, loop through, and load\n",
    "# Note that data.dat is a weird ragged file\n",
    "coverfiles = glob.glob(os.path.join(oh_path, 'all', '*.csv'))\n",
    "\n",
    "frames = []\n",
    "\n",
    "for f in coverfiles:\n",
    "    # Read csv and append to frames list (note these csvs have extra spaces)\n",
    "    df = pd.read_csv(f, na_values=['ERROR','.', ' ', ''], skipinitialspace=True)\n",
    "    # Lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    #Strip some whitespace in column index\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate dfs into one\n",
    "ohcover_df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "# Rename cols and strip whitespace in columns\n",
    "ohcover_df = ohcover_df.rename(columns={'key':'image_num_orig', 'image':'image_fname'})\n",
    "ohcover_df['image_fname'] = ohcover_df['image_fname'].str.strip()\n",
    "ohcover_df['comment'] = ohcover_df['comment'].str.strip()\n",
    "print(ohcover_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5953, 75)\n",
      "35 35\n",
      "(5953, 59)\n"
     ]
    }
   ],
   "source": [
    "# Drop some columns and rows with errors\n",
    "# Drop rows with ZERO or ERROR in them (should be about 32)\n",
    "ohcover_df = ohcover_df[~ohcover_df.image_fname.str.contains('ZERO')]\n",
    "#ohcover_df = ohcover_df[~ohcover_df['%shrub'].str.contains('ERROR', na=False)] # Now handled on read\n",
    "# These columns show up from trailing commas but are empty\n",
    "ohcover_df = ohcover_df.drop(columns=['unnamed: 35','unnamed: 31'])\n",
    "print(ohcover_df.shape)\n",
    "\n",
    "# Extract the percent columns and the count columns (should be 35 of each)\n",
    "count_cols = [c for c in ohcover_df.columns[5:] if r'%' not in c]\n",
    "pct_cols = [c for c in ohcover_df.columns[5:] if r'%' in c]\n",
    "print(len(count_cols), len(pct_cols))\n",
    "\n",
    "# We're going to drop some because they were discontinued by the techs\n",
    "# and sum to zero:\n",
    "ohcover_df[pct_cols].sum()\n",
    "ohcover_df[count_cols].sum()\n",
    "# One column has one observation - lump it into the unkgrs column\n",
    "test = ohcover_df['grsleaf'] > 0\n",
    "ohcover_df.loc[test, 'unkgrs'] = ohcover_df.loc[test, 'grsleaf']\n",
    "ohcover_df.loc[test, '%unkgrs'] = ohcover_df.loc[test, '%grsleaf']\n",
    "\n",
    "# Now remove the empty columns\n",
    "dropcols = ['%biocrus','%termite','%seedlin','%antmnd','%grsanl','%grsunk','%grsleaf','%grsbse',\n",
    "    'biocrus','termite','seedlin','antmnd','grsanl','grsunk','grsleaf','grsbse']\n",
    "ohcover_df = ohcover_df.drop(columns=dropcols)\n",
    "# Also remove from our count and pct lists\n",
    "count_cols = [c for c in count_cols if c not in dropcols]\n",
    "pct_cols = [c for c in pct_cols if c not in dropcols]\n",
    "\n",
    "print(ohcover_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_num_orig', 'image_fname', 'block', 'plot', 'microplot',\n",
      "       'image_date', 'photo_type', 'comment', 'gridsize', 'actual', 'shrub',\n",
      "       '%shrub', 's-shrub', '%s-shrub', 'forb', '%forb', 'litter', '%litter',\n",
      "       'soil', '%soil', 'conmod', '%conmod', 'rock', '%rock', 'mupo', '%mupo',\n",
      "       'aris', '%aris', 'dapu', '%dapu', 'boer', '%boer', 'p-grass',\n",
      "       '%p-grass', 'spor', '%spor', 'a-grass', '%a-grass', 'unkgrs', '%unkgrs',\n",
      "       'shrubd', '%shrubd', 's-shrd', '%s-shrd', 'mupod', '%mupod', 'arisd',\n",
      "       '%arisd', 'dapud', '%dapud', 'boerd', '%boerd', 'pgrasd', '%pgrasd',\n",
      "       'spord', '%spord', 'forbd', '%forbd', 'agrasd', '%agrasd', 'ungrsd',\n",
      "       '%ungrsd', 'outside', '%outside'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Extract some categorical and date columns from filenames\n",
    "# Split the strings in \"Image\" by - and _\n",
    "\n",
    "splits1 = ohcover_df['image_fname'].str.split('[-_]', expand=True)\n",
    "# Extract the block, plot, and microplots\n",
    "ohcover_df['block'] = splits1[0].str.extract('(\\d+)')\n",
    "ohcover_df['plot']  = splits1[0].str.extract('([a-zA-Z]+)')\n",
    "ohcover_df['microplot'] = splits1[1].str.extract('(\\d+)')\n",
    "# There are two photo naming formats with dates\n",
    "# Extract (2 re exp's) dates, standardize, convert to datetimes\n",
    "ohcover_df['image_date'] = ohcover_df.image_fname.str.extract(r'(20\\d{2}-\\d{2}-\\d{2}|20\\d{6})')\n",
    "ohcover_df['image_date'] = ohcover_df.image_date.str.replace('-','')\n",
    "ohcover_df['image_date'] = pd.to_datetime(ohcover_df.image_date, format='%Y%m%d')\n",
    "ohcover_df['photo_type'] = 'oh'\n",
    "\n",
    "#an error\n",
    "ohcover_df.loc[ohcover_df['plot']=='a','plot'] = 'A'\n",
    "\n",
    "# Reorder columns\n",
    "cols = list(ohcover_df.columns)\n",
    "ohcover_df = ohcover_df[cols[0:2] + cols[-5:] + cols[2:-5]]\n",
    "print(ohcover_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5953, 64)\n",
      "(160731, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_num_orig</th>\n",
       "      <th>image_fname</th>\n",
       "      <th>block</th>\n",
       "      <th>plot</th>\n",
       "      <th>microplot</th>\n",
       "      <th>image_date</th>\n",
       "      <th>photo_type</th>\n",
       "      <th>gridsize</th>\n",
       "      <th>actual</th>\n",
       "      <th>cover_type</th>\n",
       "      <th>cover_count</th>\n",
       "      <th>cover_percent</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10A-10O_20130221_IMG_1123404r.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>oh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>shrub</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10A-1O_20130221_IMG_1123394r.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>oh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>shrub</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10A-2O_20130221_IMG_1123395r.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>oh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>shrub</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10A-3O_20130221_IMG_1123396r.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>oh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>shrub</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10A-4O_20130221_IMG_1123397r.jpg</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-02-21</td>\n",
       "      <td>oh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>shrub</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_num_orig                        image_fname block plot microplot  \\\n",
       "0               1  10A-10O_20130221_IMG_1123404r.jpg    10    A        10   \n",
       "1               2   10A-1O_20130221_IMG_1123394r.jpg    10    A         1   \n",
       "2               3   10A-2O_20130221_IMG_1123395r.jpg    10    A         2   \n",
       "3               4   10A-3O_20130221_IMG_1123396r.jpg    10    A         3   \n",
       "4               5   10A-4O_20130221_IMG_1123397r.jpg    10    A         4   \n",
       "\n",
       "  image_date photo_type  gridsize  actual cover_type  cover_count  \\\n",
       "0 2013-02-21         oh     100.0   100.0      shrub          0.0   \n",
       "1 2013-02-21         oh     100.0   100.0      shrub          0.0   \n",
       "2 2013-02-21         oh     100.0   100.0      shrub          0.0   \n",
       "3 2013-02-21         oh     100.0   100.0      shrub          0.0   \n",
       "4 2013-02-21         oh     100.0   100.0      shrub          0.0   \n",
       "\n",
       "   cover_percent comment  \n",
       "0            0.0     NaN  \n",
       "1            0.0     NaN  \n",
       "2            0.0     NaN  \n",
       "3            0.0     NaN  \n",
       "4            0.0     NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to reshape this so its not insane columns to describe\n",
    "\n",
    "# Melt counts and percents into nearly the same dataframe with different value columns\n",
    "print(ohcover_df.shape)\n",
    "counts = ohcover_df.melt(id_vars=ohcover_df.columns[0:10], value_vars=count_cols,\n",
    "                       var_name='cover_type', value_name='cover_count')\n",
    "pcts = ohcover_df.melt(id_vars=ohcover_df.columns[0:10], value_vars=pct_cols,\n",
    "                       var_name='cover_type', value_name='cover_percent')\n",
    "# Need to make the cover_type column match, remove %\n",
    "pcts['cover_type'] = pcts['cover_type'].str.replace('%', '')\n",
    "# Now merge them\n",
    "ohcover_rshp = counts.merge(pcts, how='left')\n",
    "\n",
    "# Move the comment column to the end (easier to read in xcel)\n",
    "colcom = ohcover_rshp.pop('comment')\n",
    "ohcover_rshp.insert(len(ohcover_rshp.columns), 'comment', colcom)\n",
    "\n",
    "print(ohcover_rshp.shape)\n",
    "ohcover_rshp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "ohcover_rshp.to_csv(os.path.join(dest_path2,'jrn413005_oh_cover_data.csv'), index=False, na_rep='NA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare photos analyzed with inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13101, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohphotos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ohcover_df['image_fname'].isin(ohphotos_df['fname']).sum())\n",
    "# Currently there seem to be ~124 photos missing\n",
    "ohcover_df.shape[0] - ohcover_df['image_fname'].isin(ohphotos_df['fname']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a listing of ohcover Images (~6000) that are not present in the \n",
    "# overhead photo inventory (of ~13000)\n",
    "missing = ohcover_df.loc[~ohcover_df['image_fname'].isin(ohphotos_df['fname']),\n",
    "    ['image_num_orig', 'image_fname', 'block', 'plot', 'microplot', 'image_date', 'photo_type']]\n",
    "\n",
    "missing.to_csv(os.path.join(dest_path1, 'missing_ohcover_photos.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip up the photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a year column\n",
    "ohcover_df['imageyear'] = ohcover_df['image_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013.0\n",
      "Cover data lists 597 photos, 597 photo paths found in archive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014.0\n",
      "Cover data lists 597 photos, 597 photo paths found in archive\n",
      "2015.0\n",
      "Cover data lists 589 photos, 589 photo paths found in archive\n",
      "2016.0\n",
      "Cover data lists 589 photos, 589 photo paths found in archive\n",
      "2017.0\n",
      "Cover data lists 592 photos, 592 photo paths found in archive\n",
      "2018.0\n",
      "Cover data lists 597 photos, 598 photo paths found in archive\n",
      "2019.0\n",
      "Cover data lists 599 photos, 602 photo paths found in archive\n",
      "2020.0\n",
      "Cover data lists 597 photos, 596 photo paths found in archive\n",
      "2021.0\n",
      "Cover data lists 597 photos, 499 photo paths found in archive\n",
      "2022.0\n",
      "Cover data lists 597 photos, 595 photo paths found in archive\n",
      "                          image_fname block plot microplot image_date  \\\n",
      "5977  15D_5O_H_20220808_IMG_7899r.jpg    15    D         5 2022-08-08   \n",
      "5978  15D_6O_H_20220808_IMG_7900r.jpg    15    D         6 2022-08-08   \n",
      "5979  15D_7O_H_20220808_IMG_7901r.jpg    15    D         7 2022-08-08   \n",
      "5980  15D_8O_H_20220808_IMG_7902r.jpg    15    D         8 2022-08-08   \n",
      "5981  15D_9O_H_20220808_IMG_7903r.jpg    15    D         9 2022-08-08   \n",
      "\n",
      "     photo_type       archive_fname  \\\n",
      "5977         oh  oh_photos_2022.zip   \n",
      "5978         oh  oh_photos_2022.zip   \n",
      "5979         oh  oh_photos_2022.zip   \n",
      "5980         oh  oh_photos_2022.zip   \n",
      "5981         oh  oh_photos_2022.zip   \n",
      "\n",
      "                                       archive_relpath  \n",
      "5977  Block-15/cropped/15D_5O_H_20220808_IMG_7899r.jpg  \n",
      "5978  Block-15/cropped/15D_6O_H_20220808_IMG_7900r.jpg  \n",
      "5979  Block-15/cropped/15D_7O_H_20220808_IMG_7901r.jpg  \n",
      "5980  Block-15/cropped/15D_8O_H_20220808_IMG_7902r.jpg  \n",
      "5981  Block-15/cropped/15D_9O_H_20220808_IMG_7903r.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Create a directory file to populate\n",
    "photo_archive_dir = pd.DataFrame(ohcover_df[['image_fname', 'block', 'plot', 'microplot', 'image_date', 'photo_type']])\n",
    "photo_archive_dir['archive_fname'] = 'None'\n",
    "photo_archive_dir['archive_relpath'] = 'None'\n",
    "oh_dups_used = pd.DataFrame() # Empty dataframe for duplicates\n",
    "\n",
    "# Subset by year, zip up files, fill directory, and write to JORNADA_IM directory\n",
    "import zipfile\n",
    "for y in ohcover_df.imageyear.dropna().unique():\n",
    "    print(y)\n",
    "    # Subset cover dataset by year and get paths from photo inventory\n",
    "    subset = ohcover_df.loc[ohcover_df.imageyear==y, 'image_fname']\n",
    "    paths = ohphotos_df.loc[ohphotos_df.fname.isin(subset), 'pathname']\n",
    "    print('Cover data lists {0} photos, {1} photo paths found in archive'.format(\n",
    "        len(subset), len(paths)))\n",
    "    # If there are duplicate photos what are they?\n",
    "    if len(subset) < len(paths):\n",
    "        test = ohphotos_df.loc[ohphotos_df.fname.isin(subset), :]\n",
    "        oh_dups_used = pd.concat([oh_dups_used, test[test.duplicated('fname', keep=False) == True]])\n",
    "    # Get the parent directory of all the paths in paths (common prefix, then parent dir of that)\n",
    "    parentdir = os.path.dirname(os.path.commonprefix(paths.to_list()))\n",
    "    # Create a zipfile\n",
    "    zfile_path = os.path.join(dest_path1, 'oh_photos_{0}.zip'.format(str(int(y))))\n",
    "    #zfile = zipfile.ZipFile(zfile_path, \"w\")\n",
    "    for p in paths:\n",
    "        test = photo_archive_dir.image_fname==os.path.basename(p)\n",
    "        photo_archive_dir.loc[test, 'archive_fname'] = 'oh_photos_{0}.zip'.format(str(int(y)))\n",
    "        photo_archive_dir.loc[test, 'archive_relpath'] = os.path.relpath(p,start=parentdir)\n",
    "        # Write each file to zfile using a relative path starting at parentdir\n",
    "    #    zfile.write(p, os.path.relpath(p,start=parentdir), compress_type=zipfile.ZIP_DEFLATED)\n",
    "    #zfile.close()\n",
    "\n",
    "print(photo_archive_dir.tail())\n",
    "photo_archive_dir.to_csv(os.path.join(dest_path1, 'jrn413004_photo_archive_dir.csv'), index=False, na_rep='NA')\n",
    "oh_dups_used.to_csv(os.path.join(dest_path1, 'oh_dups_used.csv'), na_rep='NA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
